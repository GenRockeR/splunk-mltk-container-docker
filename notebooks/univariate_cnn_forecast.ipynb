{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Splunk Machine Learning Toolkit Container for TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains an example workflow for how to forecast a univariate time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# global constants\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)\n",
    "print(\"TensorFlow version: \" + tf.__version__)\n",
    "print(\"Keras version: \" + keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a prepared dataset into this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| inputlookup internet_traffic.csv \n",
    "| timechart span=120min avg(\"bits_transferred\") as bits_transferred \n",
    "| eval bits_transferred=round(bits_transferred) \n",
    "| fit RobustScaler bits_transferred \n",
    "| fit MLTKContainer algo=forecast_rnn_internet_traffic mode=stage epochs=100 batch_size=3 RS_bits_transferred _time by RS_bits_transferred into app:internet_traffic_forecast_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"internet_traffic_forecast_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "df, param = stage(\"internet_traffic_forecast_model\")\n",
    "print(df[0:1])\n",
    "print(df.shape)\n",
    "print(str(param))\n",
    "param['options']['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "# params: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "def init(df,param):\n",
    "    # Collect variables\n",
    "    model_batch_size = 3\n",
    "    n_features = 1\n",
    "    hidden_layers = 50\n",
    "    activation_func = 'sigmoid'\n",
    "    if 'options' in param:\n",
    "        if 'params' in param['options']:\n",
    "            if 'batch_size' in param['options']['params']:\n",
    "                model_batch_size = int(param['options']['params']['batch_size'])\n",
    "            if 'hidden_layers' in param['options']['params']:\n",
    "                hidden_layers = int(param['options']['params']['hidden_layers'])\n",
    "            if 'activation' in param['options']['params']:\n",
    "                activation_func = param['options']['params']['activation']\n",
    "    \n",
    "    # define model\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Conv1D(filters=64, kernel_size=2, activation=activation_func, input_shape=(model_batch_size, n_features)))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(hidden_layers,activation=activation_func))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test mltkc_stage_create_model\n",
    "model = init(df,param)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# returns a fit info json object\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, batch_size):\n",
    "    \n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + batch_size\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def fit(model,df,param):\n",
    "    returns = {}\n",
    "    \n",
    "    # Collect variables from param file\n",
    "    model_epochs = 100\n",
    "    model_batch_size = 3\n",
    "    holdback = 30\n",
    "    if 'options' in param:\n",
    "        if 'params' in param['options']:\n",
    "            if 'epochs' in param['options']['params']:\n",
    "                model_epochs = int(param['options']['params']['epochs'])\n",
    "            if 'batch_size' in param['options']['params']:\n",
    "                model_batch_size = int(param['options']['params']['batch_size'])\n",
    "            if 'holdback' in param['options']['params']:\n",
    "                holdback = int(param['options']['params']['holdback'])\n",
    "    \n",
    "    \n",
    "    # flatten data frame into an array and extract the training set\n",
    "    full_data = df[param['options']['split_by']].values.tolist()\n",
    "    train_set = list(full_data[:len(full_data)-holdback])\n",
    "    \n",
    "    # split data into samples\n",
    "    X, y = split_sequence(train_set, model_batch_size)\n",
    "    # reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "    n_features = 1\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    \n",
    "\n",
    "    # connect model training to tensorboard\n",
    "    log_dir=\"/srv/notebooks/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    # run the training\n",
    "    returns['fit_history'] = model.fit(x=X,\n",
    "                                       y=y, \n",
    "                                       verbose=2, \n",
    "                                       epochs=model_epochs,\n",
    "                                       shuffle=False,\n",
    "                                       callbacks=[tensorboard_callback])\n",
    "    # memorize parameters\n",
    "    returns['model_epochs'] = model_epochs\n",
    "    returns['model_batch_size'] = model_batch_size\n",
    "    returns['model_loss_acc'] = model.evaluate(x = X, y = y)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = fit(model,df,param)\n",
    "print(returns['model_loss_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "def apply(model,df,param):\n",
    "    \n",
    "    # Collect variables\n",
    "    model_batch_size = 3\n",
    "    future_steps = 30\n",
    "    holdback = 30\n",
    "    if 'options' in param:\n",
    "        if 'params' in param['options']:\n",
    "            if 'batch_size' in param['options']['params']:\n",
    "                model_batch_size = int(param['options']['params']['batch_size'])\n",
    "            if 'future_steps' in param['options']['params']:\n",
    "                future_steps = int(param['options']['params']['future_steps'])\n",
    "            if 'holdback' in param['options']['params']:\n",
    "                holdback = int(param['options']['params']['holdback'])\n",
    "    \n",
    "    # select training data\n",
    "    X = df[param['options']['split_by']].values.tolist()\n",
    "    \n",
    "    test_set = list(X[len(X)-holdback-model_batch_size:])\n",
    "    predictions = list(X[:len(X)-holdback])\n",
    "    \n",
    "    # generate forecast\n",
    "    for i in range(0, holdback+future_steps):\n",
    "        if i<holdback:\n",
    "            X_batch = np.asarray(test_set[i:i+model_batch_size]).reshape(1,model_batch_size,1)\n",
    "            y_pred = model.predict(x = X_batch, verbose=1)\n",
    "            predictions.append(list(y_pred[0]))\n",
    "        else:\n",
    "            X_batch = np.asarray(test_set[i:i+model_batch_size]).reshape(1,model_batch_size,1)\n",
    "            y_pred = model.predict(x = X_batch, verbose=1)\n",
    "            predictions.append(list(y_pred[0]))\n",
    "            test_set.append(y_pred)\n",
    "            \n",
    "    # append predictions to time series to return a data frame\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instance = apply(model,df,param)\n",
    "print(len(training_instance))\n",
    "training_instance[583:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def save(model,name):\n",
    "    # save keras model to hdf5 file\n",
    "    # https://www.tensorflow.org/beta/tutorials/keras/save_and_restore_models\n",
    "    model.save(MODEL_DIRECTORY + name + \".h5\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def load(name):\n",
    "    model = keras.models.load_model(MODEL_DIRECTORY + name + \".h5\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "def summary(model=None):\n",
    "    returns = {\"version\": {\"tensorflow\": tf.__version__, \"keras\": keras.__version__} }\n",
    "    if model is not None:\n",
    "        # Save keras model summary to string:\n",
    "        s = []\n",
    "        model.summary(print_fn=lambda x: s.append(x+'\\n'))\n",
    "        returns[\"summary\"] = ''.join(s)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
